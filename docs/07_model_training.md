# 7. ëª¨ë¸ í•™ìŠµ (Model Training)

## ğŸ¯ ëª¨ë¸ í•™ìŠµ ê°œìš”

ì¶”ì¶œí•œ íŠ¹ì§•ë“¤ë¡œ **Gradient Boosted Trees (GBT)** ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

```
[ì…ë ¥] íŠ¹ì§• ë°°ì—´ (789ê°œ íŠ¹ì§•)  +  ë ˆì´ë¸” (0 or 1)
           â†“
    [GBT ëª¨ë¸ í•™ìŠµ]
           â†“
[ì¶œë ¥] í–‰ì„± íƒì§€ ë¶„ë¥˜ê¸°
```

## ğŸŒ² LightGBM ì„¤ì •

### ê¸°ë³¸ ì„¤ì •

```python
import lightgbm as lgb

# ëª¨ë¸ ìƒì„±
model = lgb.LGBMClassifier(
    objective='binary',       # ì´ì§„ ë¶„ë¥˜
    metric='auc',            # í‰ê°€ ì§€í‘œ: AUC
    boosting_type='gbdt',    # Gradient Boosting Decision Tree
    n_estimators=100,        # íŠ¸ë¦¬ ê°œìˆ˜
    max_depth=5,             # íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´
    learning_rate=0.1,       # í•™ìŠµë¥ 
    num_leaves=31,           # ë¦¬í”„ ë…¸ë“œ ìˆ˜
    min_child_samples=20,    # ë¦¬í”„ì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    subsample=0.8,           # í–‰ ìƒ˜í”Œë§ ë¹„ìœ¨
    colsample_bytree=0.8,    # ì—´ ìƒ˜í”Œë§ ë¹„ìœ¨
    random_state=42,         # ì¬í˜„ì„±
    n_jobs=-1                # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
)
```

## ğŸ“Š í•™ìŠµ ë°ì´í„° ì¤€ë¹„

### ë°ì´í„° ë¶„í• 

```python
from sklearn.model_selection import train_test_split

# ì „ì²´ ë°ì´í„°
X = features_scaled  # (N, 789)
y = labels          # (N,) - 0 or 1

# í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.1, random_state=42, stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.111, random_state=42, stratify=y_temp
)

print(f"í•™ìŠµ ì„¸íŠ¸: {X_train.shape}")  # (80%, 789)
print(f"ê²€ì¦ ì„¸íŠ¸: {X_val.shape}")    # (10%, 789)
print(f"í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test.shape}")  # (10%, 789)
```

### í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬

```python
from collections import Counter

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
print("í´ë˜ìŠ¤ ë¶„í¬:")
print(f"  í•™ìŠµ: {Counter(y_train)}")
print(f"  ê²€ì¦: {Counter(y_val)}")
print(f"  í…ŒìŠ¤íŠ¸: {Counter(y_test)}")

# TESS ì˜ˆì‹œ:
# í•™ìŠµ: {0: 11000, 1: 400}  â† ì‹¬í•œ ë¶ˆê· í˜•!
# ê²€ì¦: {0: 1400, 1: 50}
# í…ŒìŠ¤íŠ¸: {0: 1400, 1: 50}

# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì„¤ì •
from sklearn.utils.class_weight import compute_sample_weight

sample_weights = compute_sample_weight(
    class_weight='balanced',
    y=y_train
)

# ë˜ëŠ” ëª¨ë¸ì—ì„œ ì§ì ‘ ì„¤ì •
model = lgb.LGBMClassifier(
    class_weight='balanced',  # ìë™ ê°€ì¤‘ì¹˜ ì¡°ì •
    ...
)
```

## ğŸ”§ 10-Fold êµì°¨ ê²€ì¦

### êµì°¨ ê²€ì¦ êµ¬í˜„

```python
from sklearn.model_selection import StratifiedKFold
import numpy as np

# 10-Fold CV
kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# ê²°ê³¼ ì €ì¥
cv_scores = []
cv_models = []

for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train, y_train)):
    print(f"\n========== Fold {fold + 1}/10 ==========")

    # ë°ì´í„° ë¶„í• 
    X_tr = X_train[train_idx]
    y_tr = y_train[train_idx]
    X_vl = X_train[val_idx]
    y_vl = y_train[val_idx]

    # ëª¨ë¸ í•™ìŠµ
    model = lgb.LGBMClassifier(**params)
    model.fit(
        X_tr, y_tr,
        eval_set=[(X_vl, y_vl)],
        eval_metric='auc',
        early_stopping_rounds=50,
        verbose=100
    )

    # ê²€ì¦ ì ìˆ˜
    y_pred = model.predict_proba(X_vl)[:, 1]
    auc = roc_auc_score(y_vl, y_pred)
    cv_scores.append(auc)
    cv_models.append(model)

    print(f"Fold {fold + 1} AUC: {auc:.4f}")

# í‰ê·  ì„±ëŠ¥
print(f"\ní‰ê·  AUC: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")
```

### Early Stopping

```python
# ê³¼ì í•© ë°©ì§€: ê²€ì¦ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¤‘ë‹¨
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=50,  # 50 ì—í­ ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
    verbose=100
)

print(f"ìµœì  ë°˜ë³µ íšŸìˆ˜: {model.best_iteration_}")
```

## ğŸ›ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

### 1ë‹¨ê³„: AUC ìµœëŒ€í™”

```python
from sklearn.model_selection import GridSearchCV

# íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„
param_grid = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'num_leaves': [15, 31, 63, 127],
    'min_child_samples': [10, 20, 30],
    'subsample': [0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.7, 0.8, 0.9, 1.0]
}

# Grid Search
grid_search = GridSearchCV(
    estimator=lgb.LGBMClassifier(),
    param_grid=param_grid,
    scoring='roc_auc',
    cv=StratifiedKFold(n_splits=10),
    n_jobs=-1,
    verbose=2
)

grid_search.fit(X_train, y_train)

# ìµœì  íŒŒë¼ë¯¸í„°
best_params = grid_search.best_params_
best_score = grid_search.best_score_

print(f"ìµœì  AUC: {best_score:.4f}")
print(f"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
```

### 2ë‹¨ê³„: ì„ê³„ê°’ ìµœì í™”

```python
# ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡
model = lgb.LGBMClassifier(**best_params)
model.fit(X_train, y_train)

# ê²€ì¦ ì„¸íŠ¸ì—ì„œ í™•ë¥  ì˜ˆì¸¡
y_proba = model.predict_proba(X_val)[:, 1]

# ë‹¤ì–‘í•œ ì„ê³„ê°’ ì‹œë„
thresholds = np.arange(0.1, 0.9, 0.05)
best_threshold = 0.5
best_recall = 0

for threshold in thresholds:
    y_pred = (y_proba >= threshold).astype(int)

    recall = recall_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred)

    # Recall ìµœëŒ€í™” (ë‹¨, Precision > 0.7 ìœ ì§€)
    if recall > best_recall and precision > 0.7:
        best_recall = recall
        best_threshold = threshold

print(f"ìµœì  ì„ê³„ê°’: {best_threshold:.2f}")
print(f"Recall: {best_recall:.4f}")
```

## ğŸ“ˆ í•™ìŠµ ê³¼ì • ì‹œê°í™”

### í•™ìŠµ ê³¡ì„ 

```python
import matplotlib.pyplot as plt

# í•™ìŠµ ì´ë ¥
history = model.evals_result_

# ê·¸ë˜í”„
plt.figure(figsize=(12, 5))

# AUC
plt.subplot(1, 2, 1)
plt.plot(history['training']['auc'], label='Train')
plt.plot(history['valid_1']['auc'], label='Validation')
plt.xlabel('Iteration')
plt.ylabel('AUC')
plt.title('AUC over Iterations')
plt.legend()
plt.grid(True)

# Loss
plt.subplot(1, 2, 2)
plt.plot(history['training']['binary_logloss'], label='Train')
plt.plot(history['valid_1']['binary_logloss'], label='Validation')
plt.xlabel('Iteration')
plt.ylabel('Binary Log Loss')
plt.title('Loss over Iterations')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### íŠ¹ì§• ì¤‘ìš”ë„

```python
# íŠ¹ì§• ì¤‘ìš”ë„
feature_importance = model.feature_importances_

# ìƒìœ„ 20ê°œ
top_20_idx = np.argsort(feature_importance)[-20:]
top_20_names = feature_names[top_20_idx]
top_20_scores = feature_importance[top_20_idx]

# ì‹œê°í™”
plt.figure(figsize=(10, 8))
plt.barh(range(20), top_20_scores)
plt.yticks(range(20), top_20_names, fontsize=8)
plt.xlabel('Importance')
plt.title('Top 20 Most Important Features')
plt.tight_layout()
plt.show()
```

## ğŸ¯ ìµœì¢… ëª¨ë¸ í•™ìŠµ

### ì „ì²´ í•™ìŠµ ë°ì´í„°ë¡œ ì¬í•™ìŠµ

```python
# ìµœì  íŒŒë¼ë¯¸í„°ì™€ ì„ê³„ê°’ í™•ì •
final_params = {
    **best_params,
    'n_estimators': model.best_iteration_  # Early stopping ë°˜ì˜
}

# í•™ìŠµ+ê²€ì¦ ë°ì´í„° ê²°í•©
X_full_train = np.vstack([X_train, X_val])
y_full_train = np.hstack([y_train, y_val])

# ìµœì¢… ëª¨ë¸ í•™ìŠµ
final_model = lgb.LGBMClassifier(**final_params)
final_model.fit(X_full_train, y_full_train)

print("ìµœì¢… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
```

## ğŸ’¾ ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ

```python
import pickle
import joblib

# ë°©ë²• 1: pickle
with open('exoplanet_model.pkl', 'wb') as f:
    pickle.dump(final_model, f)

# ë°©ë²• 2: joblib (ë” íš¨ìœ¨ì )
joblib.dump(final_model, 'exoplanet_model.joblib')

# ë¡œë“œ
model_loaded = joblib.load('exoplanet_model.joblib')

# ì˜ˆì¸¡
y_pred = model_loaded.predict(X_test)
```

## ğŸ”„ ì™„ì „í•œ í•™ìŠµ íŒŒì´í”„ë¼ì¸

```python
class ExoplanetTrainer:
    """ì™¸ê³„í–‰ì„± íƒì§€ ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""

    def __init__(self, params=None):
        self.params = params or self._default_params()
        self.model = None
        self.threshold = 0.5
        self.scaler = None

    def _default_params(self):
        return {
            'objective': 'binary',
            'metric': 'auc',
            'n_estimators': 100,
            'max_depth': 5,
            'learning_rate': 0.1,
            'num_leaves': 31,
            'random_state': 42
        }

    def train(self, X_train, y_train, X_val=None, y_val=None):
        """ëª¨ë¸ í•™ìŠµ"""
        self.model = lgb.LGBMClassifier(**self.params)

        if X_val is not None:
            self.model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                early_stopping_rounds=50,
                verbose=100
            )
        else:
            self.model.fit(X_train, y_train)

        return self

    def optimize_threshold(self, X_val, y_val, target_recall=0.9):
        """ì„ê³„ê°’ ìµœì í™”"""
        y_proba = self.model.predict_proba(X_val)[:, 1]

        best_threshold = 0.5
        best_precision = 0

        for threshold in np.arange(0.1, 0.9, 0.01):
            y_pred = (y_proba >= threshold).astype(int)
            recall = recall_score(y_val, y_pred)
            precision = precision_score(y_val, y_pred)

            if recall >= target_recall and precision > best_precision:
                best_precision = precision
                best_threshold = threshold

        self.threshold = best_threshold
        print(f"ìµœì  ì„ê³„ê°’: {self.threshold:.3f}")
        return self

    def predict(self, X):
        """ì˜ˆì¸¡"""
        y_proba = self.model.predict_proba(X)[:, 1]
        y_pred = (y_proba >= self.threshold).astype(int)
        return y_pred

    def predict_proba(self, X):
        """í™•ë¥  ì˜ˆì¸¡"""
        return self.model.predict_proba(X)[:, 1]

    def save(self, filepath):
        """ëª¨ë¸ ì €ì¥"""
        joblib.dump({
            'model': self.model,
            'threshold': self.threshold,
            'params': self.params
        }, filepath)

    @classmethod
    def load(cls, filepath):
        """ëª¨ë¸ ë¡œë“œ"""
        data = joblib.load(filepath)
        trainer = cls(params=data['params'])
        trainer.model = data['model']
        trainer.threshold = data['threshold']
        return trainer

# ì‚¬ìš© ì˜ˆì‹œ
trainer = ExoplanetTrainer()
trainer.train(X_train, y_train, X_val, y_val)
trainer.optimize_threshold(X_val, y_val, target_recall=0.95)
trainer.save('final_model.joblib')

# ì˜ˆì¸¡
y_pred = trainer.predict(X_test)
```

## ğŸ“ ìš”ì•½

- **ëª¨ë¸**: LightGBM (Gradient Boosted Trees)
- **êµì°¨ ê²€ì¦**: 10-fold StratifiedKFold
- **ìµœì í™”**: 2ë‹¨ê³„ (AUC â†’ Recall)
- **Early Stopping**: ê³¼ì í•© ë°©ì§€
- **í´ë˜ìŠ¤ ë¶ˆê· í˜•**: ê°€ì¤‘ì¹˜ ì¡°ì •
- **ì €ì¥**: joblibë¡œ ëª¨ë¸ ì €ì¥

## ğŸ¤” í€´ì¦ˆë¡œ í™•ì¸í•˜ê¸°

1. ì™œ êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•˜ë‚˜ìš”?
   <details>
   <summary>ë‹µ ë³´ê¸°</summary>
   ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ ì„±ëŠ¥ì„ ë” ì‹ ë¢°ì„± ìˆê²Œ í‰ê°€í•˜ê¸° ìœ„í•´
   </details>

2. Early Stoppingì´ë€?
   <details>
   <summary>ë‹µ ë³´ê¸°</summary>
   ê²€ì¦ ì„±ëŠ¥ì´ ì¼ì • ê¸°ê°„ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµì„ ì¤‘ë‹¨í•˜ì—¬ ê³¼ì í•© ë°©ì§€
   </details>

3. ì„ê³„ê°’ì„ ì™œ ì¡°ì •í•˜ë‚˜ìš”?
   <details>
   <summary>ë‹µ ë³´ê¸°</summary>
   Recallì„ ìµœëŒ€í™”í•˜ì—¬ í–‰ì„±ì„ ìµœëŒ€í•œ ë§ì´ ì°¾ê¸° ìœ„í•´
   </details>

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ëª¨ë¸ í•™ìŠµì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!

ë‹¤ìŒì€ **ê²°ê³¼ ë¶„ì„** ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

ğŸ‘‰ **[ë‹¤ìŒ: ê²°ê³¼ ë¶„ì„](08_results.md)**

---

**ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?**
- [â† ì´ì „: íŠ¹ì§• ì¶”ì¶œ](06_feature_extraction.md)
- [ìš©ì–´ ì‚¬ì „](09_glossary.md)ì—ì„œ ëª¨ë¥´ëŠ” ìš©ì–´ë¥¼ ì°¾ì•„ë³´ì„¸ìš”
