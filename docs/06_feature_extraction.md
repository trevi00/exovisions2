# 6. íŠ¹ì§• ì¶”ì¶œ (Feature Extraction)

## ğŸ¯ íŠ¹ì§• ì¶”ì¶œì´ë€?

**íŠ¹ì§•(Feature)**ì€ ë°ì´í„°ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•œ ê²ƒì…ë‹ˆë‹¤.

```
ê´‘ë„ ê³¡ì„  (ì›ì‹œ ë°ì´í„°)     â†’     789ê°œì˜ ìˆ«ì (íŠ¹ì§•)
[1.0, 0.99, 0.98, ...]    â†’     [í‰ê· , í‘œì¤€í¸ì°¨, FFTê³„ìˆ˜, ...]
ì‹œê°„ ì‹œë¦¬ì¦ˆ (ë³µì¡)         â†’     ìˆ˜ì¹˜ ë°°ì—´ (ê°„ë‹¨)
```

## ğŸ”§ tsfresh ë¼ì´ë¸ŒëŸ¬ë¦¬

### ì™œ tsfreshì¸ê°€?

**tsfresh (Time Series Feature extraction)**
- ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ **ìë™ìœ¼ë¡œ** íŠ¹ì§• ì¶”ì¶œ
- **789ê°œ**ì˜ ë‹¤ì–‘í•œ íŠ¹ì§• ìƒì„±
- í†µê³„ì ìœ¼ë¡œ ê²€ì¦ëœ íŠ¹ì§•ë“¤

### ì„¤ì¹˜

```bash
pip install tsfresh
```

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from tsfresh import extract_features
from tsfresh.feature_extraction import EfficientFCParameters
import pandas as pd

# ê´‘ë„ ê³¡ì„  ë°ì´í„° ì¤€ë¹„
df = pd.DataFrame({
    'id': [1, 1, 1, 2, 2, 2],  # ê´‘ë„ ê³¡ì„  ID
    'time': [0, 1, 2, 0, 1, 2],
    'flux': [1.0, 0.99, 1.0, 1.0, 0.98, 1.0]
})

# íŠ¹ì§• ì¶”ì¶œ
features = extract_features(
    df,
    column_id='id',
    column_sort='time',
    column_value='flux',
    default_fc_parameters=EfficientFCParameters()
)

print(features.shape)  # (2, 789) - 2ê°œ ê³¡ì„ , 789ê°œ íŠ¹ì§•
```

## ğŸ“Š ì¶”ì¶œë˜ëŠ” íŠ¹ì§• ìœ í˜•

### 1. í†µê³„ì  íŠ¹ì§• (Statistical Features)

ê¸°ë³¸ì ì¸ í†µê³„ ê°’ë“¤:

```python
# ì¤‘ì‹¬ ê²½í–¥
mean                    # í‰ê· 
median                  # ì¤‘ì•™ê°’
variance                # ë¶„ì‚°
standard_deviation      # í‘œì¤€í¸ì°¨

# ë¶„í¬ í˜•íƒœ
skewness               # ì™œë„ (ë¹„ëŒ€ì¹­ì„±)
kurtosis               # ì²¨ë„ (ë¾°ì¡±í•¨)

# ìœ„ì¹˜ í†µê³„
minimum                # ìµœì†Œê°’
maximum                # ìµœëŒ€ê°’
quantile_25            # 1ì‚¬ë¶„ìœ„ìˆ˜
quantile_75            # 3ì‚¬ë¶„ìœ„ìˆ˜
```

**ì˜ˆì‹œ: ì™œë„(Skewness)**
```
ëŒ€ì¹­ ë¶„í¬ (skewness â‰ˆ 0):
    â€¢
  â€¢ â€¢ â€¢
â€¢ â€¢ â€¢ â€¢ â€¢

ì˜¤ë¥¸ìª½ ê¼¬ë¦¬ (skewness > 0):
  â€¢
â€¢ â€¢ â€¢
    â€¢ â€¢ â€¢

ì™¼ìª½ ê¼¬ë¦¬ (skewness < 0):
    â€¢ â€¢ â€¢
  â€¢ â€¢ â€¢
â€¢
```

### 2. ì—ë„ˆì§€ ë° íŒŒì›Œ íŠ¹ì§•

```python
# ì—ë„ˆì§€
absolute_energy        # Î£(xÂ²) - ì‹ í˜¸ì˜ ì´ ì—ë„ˆì§€
absolute_sum_changes   # Î£|x[i+1] - x[i]| - ë³€í™”ëŸ‰ ì´í•©

# íŒŒì›Œ
variance_larger_than_std  # ë¶„ì‚° > í‘œì¤€í¸ì°¨ì¸ ë¹„ìœ¨
```

**ì˜ˆì‹œ: Absolute Energy**
```
flux = [1.0, 0.99, 0.98, 1.0]
absolute_energy = 1.0Â² + 0.99Â² + 0.98Â² + 1.0Â² = 3.9205
```

### 3. ì£¼íŒŒìˆ˜ ì˜ì—­ íŠ¹ì§• (Frequency Domain)

FFT (Fast Fourier Transform)ë¡œ ì£¼íŒŒìˆ˜ ë¶„ì„:

```python
# FFT ê³„ìˆ˜
fft_coefficient        # í‘¸ë¦¬ì— ë³€í™˜ ê³„ìˆ˜
fft_aggregated         # ì§‘ê³„ëœ FFT íŠ¹ì„±

# íŒŒì›Œ ìŠ¤í™íŠ¸ëŸ¼
spectral_centroid      # ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬
spectral_entropy       # ìŠ¤í™íŠ¸ëŸ¼ ì—”íŠ¸ë¡œí”¼
```

**ì‹œê°ì  ì´í•´**
```
ì‹œê°„ ì˜ì—­:
amplitude
    â†‘
    â”‚ ~Â·~Â·~Â·~Â·~
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ time

ì£¼íŒŒìˆ˜ ì˜ì—­:
power
    â†‘  |
    â”‚  |    |
    â”‚  |  | |
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ frequency
       ì£¼ìš” ì£¼íŒŒìˆ˜!
```

### 4. ìê¸°ìƒê´€ íŠ¹ì§• (Autocorrelation)

ì‹ í˜¸ê°€ ìê¸° ìì‹ ê³¼ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€:

```python
autocorrelation        # ìê¸°ìƒê´€ í•¨ìˆ˜
partial_autocorrelation  # ë¶€ë¶„ ìê¸°ìƒê´€
```

**ì£¼ê¸°ì  ì‹ í˜¸ íƒì§€**
```
ì£¼ê¸°ì  ì‹ í˜¸:
â•²â•±â•²â•±â•²â•±â•²â•±  â†’ ë†’ì€ ìê¸°ìƒê´€

ëœë¤ ë…¸ì´ì¦ˆ:
Â·â€¢Â·â€¢Â·â€¢Â·â€¢  â†’ ë‚®ì€ ìê¸°ìƒê´€
```

### 5. ë³µì¡ë„ ì¸¡ì •

ì‹ í˜¸ì˜ ë³µì¡ë„ì™€ ë¬´ì‘ìœ„ì„±:

```python
# ì—”íŠ¸ë¡œí”¼
entropy                # Shannon ì—”íŠ¸ë¡œí”¼
approximate_entropy    # ê·¼ì‚¬ ì—”íŠ¸ë¡œí”¼
sample_entropy         # ìƒ˜í”Œ ì—”íŠ¸ë¡œí”¼

# ë³µì¡ë„
cid_ce                 # Complexity-Invariant Distance
```

### 6. ì„ í˜•ì„± ë° ì¶”ì„¸

```python
# ì„ í˜• ì¶”ì„¸
linear_trend           # ì„ í˜• íšŒê·€ ê¸°ìš¸ê¸°
agg_linear_trend       # ì§‘ê³„ ì„ í˜• ì¶”ì„¸

# ìê¸°íšŒê·€ ëª¨ë¸
ar_coefficient         # AR ëª¨ë¸ ê³„ìˆ˜
```

### 7. ì¹´ìš´íŠ¸ ë° ë¹„ìœ¨

```python
# ì„ê³„ê°’ ê¸°ë°˜
count_above_mean       # í‰ê· ë³´ë‹¤ í° ê°’ ê°œìˆ˜
count_below_mean       # í‰ê· ë³´ë‹¤ ì‘ì€ ê°’ ê°œìˆ˜

# ë³€í™”
number_peaks           # í”¼í¬ ê°œìˆ˜
number_crossing_m      # í‰ê·  êµì°¨ íšŸìˆ˜
```

## ğŸ“ íŠ¹ì§• ì¶”ì¶œ ì „ì²´ ê³¼ì •

### 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„

```python
import numpy as np
import pandas as pd
from tsfresh import extract_features
from tsfresh.feature_extraction import EfficientFCParameters

# ì—¬ëŸ¬ ê´‘ë„ ê³¡ì„  ì¤€ë¹„
light_curves = []
for curve_id in range(100):
    time = np.linspace(0, 10, 1000)
    flux = load_light_curve(curve_id)  # ê°€ìƒì˜ í•¨ìˆ˜

    df = pd.DataFrame({
        'id': curve_id,
        'time': time,
        'flux': flux
    })
    light_curves.append(df)

# í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©
all_curves = pd.concat(light_curves)
```

### 2ë‹¨ê³„: íŠ¹ì§• ì¶”ì¶œ

```python
# íŠ¹ì§• ì¶”ì¶œ (ì‹œê°„ ì†Œìš”: ìˆ˜ ë¶„)
features = extract_features(
    all_curves,
    column_id='id',
    column_sort='time',
    column_value='flux',
    default_fc_parameters=EfficientFCParameters()
)

print(f"Shape: {features.shape}")
# Output: Shape: (100, 789)
```

### 3ë‹¨ê³„: íŠ¹ì§• ì •ë¦¬

```python
# 1. ìƒìˆ˜ íŠ¹ì§• ì œê±°
from sklearn.feature_selection import VarianceThreshold

selector = VarianceThreshold(threshold=0)
features_filtered = selector.fit_transform(features)

# 2. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
features_filtered = pd.DataFrame(features_filtered)
features_filtered = features_filtered.fillna(features_filtered.mean())

# 3. ìŠ¤ì¼€ì¼ë§ (Robust Scaler)
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
features_scaled = scaler.fit_transform(features_filtered)

print(f"Final shape: {features_scaled.shape}")
# ì•½ 700ê°œ íŠ¹ì§•ìœ¼ë¡œ ì¶•ì†Œë¨
```

## ğŸ” ì£¼ìš” íŠ¹ì§• ë¶„ì„

### íŠ¹ì§• ì¤‘ìš”ë„ í™•ì¸

```python
import lightgbm as lgb
import matplotlib.pyplot as plt

# ëª¨ë¸ í•™ìŠµ
model = lgb.LGBMClassifier()
model.fit(X_train, y_train)

# íŠ¹ì§• ì¤‘ìš”ë„
feature_importance = model.feature_importances_
feature_names = features.columns

# ìƒìœ„ 20ê°œ íŠ¹ì§•
top_20_idx = np.argsort(feature_importance)[-20:]
top_20_features = feature_names[top_20_idx]
top_20_importance = feature_importance[top_20_idx]

# ì‹œê°í™”
plt.figure(figsize=(10, 8))
plt.barh(range(20), top_20_importance)
plt.yticks(range(20), top_20_features)
plt.xlabel('Importance')
plt.title('Top 20 Most Important Features')
plt.tight_layout()
plt.show()
```

### íŠ¹ì§• ìƒê´€ê´€ê³„ ë¶„ì„

```python
import seaborn as sns

# ìƒìœ„ 50ê°œ íŠ¹ì§•ë§Œ ì„ íƒ
top_50_features = features[top_20_features[:50]]

# ìƒê´€ê´€ê³„ í–‰ë ¬
corr_matrix = top_50_features.corr()

# íˆíŠ¸ë§µ
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, cmap='coolwarm', center=0)
plt.title('Feature Correlation Heatmap')
plt.tight_layout()
plt.show()
```

## ğŸ’» ì™„ì „í•œ íŠ¹ì§• ì¶”ì¶œ íŒŒì´í”„ë¼ì¸

```python
class FeatureExtractor:
    """ê´‘ë„ ê³¡ì„  íŠ¹ì§• ì¶”ì¶œ í´ë˜ìŠ¤"""

    def __init__(self):
        self.scaler = None
        self.feature_names = None

    def extract_and_process(self, light_curves_df):
        """
        ì „ì²´ íŠ¹ì§• ì¶”ì¶œ ë° ì²˜ë¦¬

        Parameters:
        -----------
        light_curves_df : DataFrame
            columns: ['id', 'time', 'flux']

        Returns:
        --------
        features_processed : numpy array
            ì²˜ë¦¬ëœ íŠ¹ì§• ë°°ì—´
        """
        print("1. tsfreshë¡œ íŠ¹ì§• ì¶”ì¶œ ì¤‘...")
        features = extract_features(
            light_curves_df,
            column_id='id',
            column_sort='time',
            column_value='flux',
            default_fc_parameters=EfficientFCParameters()
        )

        print(f"   ì¶”ì¶œëœ íŠ¹ì§•: {features.shape[1]}ê°œ")

        print("2. ìƒìˆ˜ íŠ¹ì§• ì œê±° ì¤‘...")
        features = self._remove_constant_features(features)
        print(f"   ë‚¨ì€ íŠ¹ì§•: {features.shape[1]}ê°œ")

        print("3. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...")
        features = self._handle_missing_values(features)

        print("4. ìŠ¤ì¼€ì¼ë§ ì¤‘...")
        features = self._scale_features(features)

        print("âœ“ íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ!")
        return features

    def _remove_constant_features(self, features):
        """ìƒìˆ˜ íŠ¹ì§• ì œê±°"""
        selector = VarianceThreshold(threshold=0)
        features_filtered = selector.fit_transform(features)
        self.feature_names = features.columns[selector.get_support()]
        return pd.DataFrame(features_filtered, columns=self.feature_names)

    def _handle_missing_values(self, features):
        """ê²°ì¸¡ì¹˜ë¥¼ í‰ê· ìœ¼ë¡œ ëŒ€ì²´"""
        return features.fillna(features.mean())

    def _scale_features(self, features):
        """Robust Scalerë¡œ ìŠ¤ì¼€ì¼ë§"""
        from sklearn.preprocessing import RobustScaler

        if self.scaler is None:
            self.scaler = RobustScaler()
            scaled = self.scaler.fit_transform(features)
        else:
            scaled = self.scaler.transform(features)

        return scaled

# ì‚¬ìš© ì˜ˆì‹œ
extractor = FeatureExtractor()
X = extractor.extract_and_process(light_curves_df)
```

## ğŸ“Š íŠ¹ì§• ì˜ˆì‹œ

ì‹¤ì œ ê´‘ë„ ê³¡ì„ ì—ì„œ ì¶”ì¶œëœ íŠ¹ì§• ì˜ˆ:

```python
# ì˜ˆì‹œ ê´‘ë„ ê³¡ì„ 
curve_id = 1
features_sample = features.loc[curve_id]

print("ì£¼ìš” íŠ¹ì§•:")
print(f"  í‰ê· : {features_sample['flux__mean']:.4f}")
print(f"  í‘œì¤€í¸ì°¨: {features_sample['flux__std']:.4f}")
print(f"  ì™œë„: {features_sample['flux__skewness']:.4f}")
print(f"  FFT ê³„ìˆ˜(0): {features_sample['flux__fft_coefficient__coeff_0']:.4f}")
print(f"  ìê¸°ìƒê´€(1): {features_sample['flux__autocorrelation__lag_1']:.4f}")

# ì¶œë ¥:
#   í‰ê· : 0.9985
#   í‘œì¤€í¸ì°¨: 0.0042
#   ì™œë„: -0.2341
#   FFT ê³„ìˆ˜(0): 0.9985
#   ìê¸°ìƒê´€(1): 0.9876
```

## ğŸ“ ìš”ì•½

- **tsfresh**: ìë™ ì‹œê³„ì—´ íŠ¹ì§• ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **789ê°œ íŠ¹ì§•**: ë‹¤ì–‘í•œ ìœ í˜•ì˜ íŠ¹ì§• ìƒì„±
- **íŠ¹ì§• ìœ í˜•**: í†µê³„, ì—ë„ˆì§€, ì£¼íŒŒìˆ˜, ìê¸°ìƒê´€, ë³µì¡ë„ ë“±
- **ì „ì²˜ë¦¬**: ìƒìˆ˜ ì œê±°, ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ìŠ¤ì¼€ì¼ë§
- **ì¤‘ìš”ë„ ë¶„ì„**: ì–´ë–¤ íŠ¹ì§•ì´ ì¤‘ìš”í•œì§€ í™•ì¸ ê°€ëŠ¥

## ğŸ¤” í€´ì¦ˆë¡œ í™•ì¸í•˜ê¸°

1. íŠ¹ì§• ì¶”ì¶œì´ ì™œ í•„ìš”í•œê°€ìš”?
   <details>
   <summary>ë‹µ ë³´ê¸°</summary>
   ê³ ì „ì  ë¨¸ì‹ ëŸ¬ë‹ì€ ì›ì‹œ ë°ì´í„°ë¥¼ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ì—†ì–´ ìˆ˜ì¹˜ë¡œ í‘œí˜„ëœ íŠ¹ì§•ì´ í•„ìš”
   </details>

2. tsfreshëŠ” ëª‡ ê°œì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ë‚˜ìš”?
   <details>
   <summary>ë‹µ ë³´ê¸°</summary>
   ì•½ 789ê°œì˜ ì‹œê³„ì—´ íŠ¹ì§•
   </details>

3. ì™œ ìŠ¤ì¼€ì¼ë§ì„ í•˜ë‚˜ìš”?
   <details>
   <summary>ë‹µ ë³´ê¸°</summary>
   íŠ¹ì§•ë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ ë²”ìœ„ë¥¼ ê°€ì§€ë¯€ë¡œ ë™ì¼í•œ ìŠ¤ì¼€ì¼ë¡œ ë§Œë“¤ì–´ ëª¨ë¸ í•™ìŠµì„ ì•ˆì •í™”
   </details>

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

íŠ¹ì§• ì¶”ì¶œì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!

ë‹¤ìŒì€ ì´ íŠ¹ì§•ë“¤ë¡œ **ëª¨ë¸ì„ í•™ìŠµ**í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

ğŸ‘‰ **[ë‹¤ìŒ: ëª¨ë¸ í•™ìŠµ](07_model_training.md)**

---

**ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?**
- [â† ì´ì „: ë°ì´í„° ì²˜ë¦¬](05_data_processing.md)
- [ìš©ì–´ ì‚¬ì „](09_glossary.md)ì—ì„œ ëª¨ë¥´ëŠ” ìš©ì–´ë¥¼ ì°¾ì•„ë³´ì„¸ìš”
